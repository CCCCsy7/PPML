{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJSOsXipEMJ5"
      },
      "source": [
        "# Differentially Private Deep Learning with Opacus\n",
        "\n",
        "Adapted from  https://github.com/pytorch/opacus/blob/main/tutorials/intro_to_advanced_features.ipynb\n",
        "\n",
        "In this document, we will investigate the advanced features of Opacus and see how to implement custom functionality.\n",
        "\n",
        "First of all, we recommend you get a GPU runtime for this Colab! You can do so by clicking on Runtime > Change Runtime Type above, and selecting GPU.\n",
        "\n",
        "First things first: let's start by installing Opacus."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "There are three components essential to DP-SGD.\n",
        "\n",
        "  \n",
        "1. The norm of the gradient value for every sample is clipped to a certain value\n",
        "  \n",
        "2. Calibrated gaussian noise is added to the resulting batch gradient to hide the individual contributions.\n",
        "\n",
        "3. Minibatches should be formed by uniform sampling, i.e. on each training step, each sample from the dataset is included with a certain probability `q`. Note, that this is different from standard approach of dataset being shuffled and split into batches: each sample has a non-zero probability of appearing multiple times in a given epoch, or not appearing at all.\n",
        "\n",
        "\n",
        "This translates into the three distinctions from standard training:\n",
        "\n",
        "1. We need to compute per sample gradients (so that we know what to clip). Currently, PyTorch autograd engine only stores gradients aggregated over a batch.\n",
        "2. We need to incorporate Poisson sampling into the training process.\n",
        "3. We need to implement gradient clipping and noise addition\n",
        "4. Finally, we need to keep an account of the privacy parameter."
      ],
      "metadata": {
        "id": "PKxUrCoWw8LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opacus\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512"
      ],
      "metadata": {
        "id": "w1xz_uvCjWpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS-bGIqAEtT-"
      },
      "source": [
        "# Usual suspects\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "# Our shiny cool lib\n",
        "import opacus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 0: Prerequistes"
      ],
      "metadata": {
        "id": "HEc5fKM4OlBd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJIU1qCUFMMG"
      },
      "source": [
        "## Let's load the training data\n",
        "Our task: **train a CIFAR10 model with differential privacy.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "wTjdBsMXYYg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlTjNJ3wGIgt"
      },
      "source": [
        "from torchvision.datasets import CIFAR10, CIFAR100\n",
        "from torchvision.transforms import Compose, Normalize, ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
        "\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "train_ds = CIFAR10('.',\n",
        "                   train=True,\n",
        "                   download=True,\n",
        "                   transform=Compose([ToTensor(), Normalize(IMAGENET_MEAN, IMAGENET_STD)])\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "test_ds = CIFAR10('.',\n",
        "                  train=False,\n",
        "                  download=True,\n",
        "                  transform=Compose([ToTensor(), Normalize(IMAGENET_MEAN, IMAGENET_STD)])\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "# Helpful for quick checks\n",
        "x, y = next(iter(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "g06Il459Ln9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HESocROn1dey"
      },
      "source": [
        "## Load pretrained model\n",
        "We load pretrained Resnet and fine-tune only the last layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ6tHY5vFBao"
      },
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "resnet_modules = list(resnet18(pretrained=True).children())\n",
        "\n",
        "backbone = nn.Sequential(*resnet_modules[:-3])\n",
        "head = nn.Sequential(*resnet_modules[-3:-1], nn.Flatten(), nn.Linear(512, 10))\n",
        "\n",
        "backbone = backbone.eval()\n",
        "head = head.train()\n",
        "\n",
        "# Quick sanity check\n",
        "\n",
        "with torch.no_grad():\n",
        "  representation = backbone(x)\n",
        "\n",
        "head(representation).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For maximal speed, we can check if CUDA is available and supported by the PyTorch installation. If GPU is available, set the device variable to your CUDA-compatible device. We can then transfer the neural network onto that device."
      ],
      "metadata": {
        "id": "6anWceHdbfNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "backbone = backbone.to(device)\n",
        "head = head.to(device)"
      ],
      "metadata": {
        "id": "QRk-8Ebtks-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next few sections, we will replicate the functionality of make_private using custom functions"
      ],
      "metadata": {
        "id": "9pM2iHiDTgyg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKa6MDuHKEqF"
      },
      "source": [
        "# validate that the model is compatible with opacus and fix any issues\n",
        "\n",
        "from opacus.validators import ModuleValidator\n",
        "head = ModuleValidator.fix(head)\n",
        "ModuleValidator.validate(head, strict=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Defining custom private components\n",
        "\n",
        "The `make_private` function we used earlier does a lot of heavy lifting.\n",
        "This image captures its overall structure. https://github.com/pytorch/opacus/blob/main/tutorials/img/make_private.png\n",
        "\n",
        "We will see how we can customize and replicate its functionality. This will be very useful for research and perhaps for your project report as well."
      ],
      "metadata": {
        "id": "NB8T8bOEOt62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1.a. Defining a private model"
      ],
      "metadata": {
        "id": "yJQsMgxtVQyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by wrapping the model with GradSampleModule - very straightforward."
      ],
      "metadata": {
        "id": "xFFXuWZSTq_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opacus import GradSampleModule\n",
        "\n",
        "head = GradSampleModule(head)\n",
        "head"
      ],
      "metadata": {
        "id": "RphqgiTHS08u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1. When implementing Poisson sampling, we need to forbid gradient accumulation. Why is this? (1 point)\n",
        "\n",
        "Answer here or on overleaf\n",
        "\n",
        "If we're using Poisson sampling, we have to forbid gradient accumulation: you'd have to call `optimizer.step()` and `zero_grad()` after every forward/backward pass."
      ],
      "metadata": {
        "id": "UGTh2HPdKQXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head.forbid_grad_accumulation()\n",
        "\n",
        "# Note comment out the code below and re-run once before running the training code.\n",
        "# Opacus seems to not like using zero_grad() outside training.\n",
        "\n",
        "# # first backward should work fine\n",
        "# with torch.no_grad():\n",
        "#     representation = backbone(x)\n",
        "# preds = head(representation)\n",
        "# preds.sum().backward()\n",
        "\n",
        "# print(\"First backward successful\")\n",
        "\n",
        "# # second should fail\n",
        "# with torch.no_grad():\n",
        "#     representation = backbone(x)\n",
        "# preds = head(representation)\n",
        "# preds.sum().backward()\n",
        "\n",
        "# head.zero_grad()"
      ],
      "metadata": {
        "id": "xHGPOxStKlXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.b. Private Data loader"
      ],
      "metadata": {
        "id": "L9JnRDB2MPEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now got to the data loader. Note, that DPDataLoader returns a brand new DataLoader, which is backed by the same dataset."
      ],
      "metadata": {
        "id": "3foUE5N3MT7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opacus.data_loader import DPDataLoader\n",
        "\n",
        "dp_data_loader = DPDataLoader.from_data_loader(train_loader, distributed=False)\n",
        "\n",
        "print(\"Is dataset the same: \", dp_data_loader.dataset == train_loader.dataset)\n",
        "print(f\"DPDataLoader length: {len(dp_data_loader)}, original: {len(train_loader)}\")\n",
        "print(\"DPDataLoader sampler: \", dp_data_loader.batch_sampler)\n",
        "\n",
        "data_loader = dp_data_loader"
      ],
      "metadata": {
        "id": "X5SiXok-SK71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main reason we need a different private loader is because we need to do Poisson sampling for our DP amplification. An interesting property of Poisson sampling, which we need to take into account, is that batch sizes are not constant. Yes, on average it'll be the same as the batch size of the original data loader, but it'll vary on every iteration:"
      ],
      "metadata": {
        "id": "juswcGAYMrEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch_sizes = []\n",
        "for x,y in dp_data_loader:\n",
        "    batch_sizes.append(len(x))\n",
        "\n",
        "plt.hist(batch_sizes)"
      ],
      "metadata": {
        "id": "BkHkB-PnMvzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.c Custom Private Optimizer\n",
        "\n",
        "Because of the variability, we can't infer the batch size from the input shape. And we need to know the batch size if we're averaging the gradients (with added noise) - under the DP assumptions sampling outcome (i.e. real batch size) does not affect the amount of noise added.\n",
        "\n",
        "So we calculate expected batch size by looking at the data loader (it'll be the same as the batch size of the original data loader, we just need to make a few extra steps in case the original data loader was initialized with custom sampler)."
      ],
      "metadata": {
        "id": "_mOH9yfANMah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opacus.optimizers import DPOptimizer\n",
        "from opacus.optimizers.optimizer import _check_processed_flag, _mark_as_processed\n",
        "from torch.distributions.laplace import Laplace\n",
        "\n",
        "sample_rate = 1 / len(data_loader)\n",
        "expected_batch_size = int(len(data_loader.dataset) * sample_rate)"
      ],
      "metadata": {
        "id": "Nqvf2N6MSFpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many non-standard use cases will involve customizing the behaviour of `DPOptimizer`. We will customize this object to implement Laplace noise. By default, the noise added is Gaussian. To do this, we need to modify the `add_noise` method."
      ],
      "metadata": {
        "id": "ZbqMAMlNP95T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See this image for an overiview of how the DPOOptimizer functions: https://github.com/pytorch/opacus/blob/main/tutorials/img/optimizer.png"
      ],
      "metadata": {
        "id": "dRtWWBWzQxXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2. What should be the loc and scale of the Laplace noise below as a function of `self.noise_multiplier` and `self.max_grad_norm` (1 point)"
      ],
      "metadata": {
        "id": "7dpCAZlE7tsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a non-private optimizer\n",
        "optimizer = torch.optim.SGD(head.parameters(), lr=0.3, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "# Define a custom class which adds laplace noise\n",
        "class LaplaceDPOptimizer(DPOptimizer):\n",
        "    def add_noise(self):\n",
        "        # FILL IN BELOW.\n",
        "        # COMPUTE `loc` and `scale` as a function of `self.noise_multiplier` and `self.max_grad_norm`\n",
        "        laplace = Laplace(loc= FILLINHERE , scale= FILLINHERE)\n",
        "        for p in self.params:\n",
        "            _check_processed_flag(p.summed_grad)\n",
        "\n",
        "            noise = laplace.sample(p.summed_grad.shape)\n",
        "            # becuse grad may be on GPU, we need send noise to GPU\n",
        "            noise = noise.to(p.summed_grad.device)\n",
        "            p.grad = p.summed_grad + noise\n",
        "\n",
        "            _mark_as_processed(p.summed_grad)\n",
        "\n",
        "\n",
        "# Convert our non-private optimizer to a private one\n",
        "optimizer = LaplaceDPOptimizer(\n",
        "    optimizer=optimizer,\n",
        "    noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        "    expected_batch_size=expected_batch_size,\n",
        ")"
      ],
      "metadata": {
        "id": "U9XAjP-SP-U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.d Privacy Accounting\n",
        "\n",
        "And now the final (and most important) piece of the puzzle - privacy accounting. We will define an accountant for our Laplace mechanism.\n",
        "\n",
        "Let us first write a function which computes the privacy parameter ɛ."
      ],
      "metadata": {
        "id": "QSqp0RUUO-iW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3. Given history (list of values of `noise_multiplier` and `q`), compute epsilon (2 points)"
      ],
      "metadata": {
        "id": "CyxuVFXN8jA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "def compute_eps(\n",
        "    history: List[Tuple[float, float, int]]\n",
        ") -> float:\n",
        "    r\"\"\"Computes Differential Privacy guarantees of the\n",
        "    Sampled Laplace Mechanism (SLM) given history.\n",
        "\n",
        "    Args: history which is a list of (q,noise_multiplier,step)\n",
        "        noise_multiplier: The ratio of the additive Laplacian noise parameter\n",
        "            to the L1-sensitivity of the function\n",
        "            to which it is added.\n",
        "        q: Sampling rate of SLM.\n",
        "        step: Current iteration step.\n",
        "\n",
        "    Returns:\n",
        "        The float value of epislon for pure DP\n",
        "    \"\"\"\n",
        "\n",
        "    for noise_multiplier, q, step in history:\n",
        "        # FILL IN HERE LOGIC TO COMPUTE EPSILON\n",
        "\n",
        "    return epsilon"
      ],
      "metadata": {
        "id": "_TCcRk44gcVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will customize the abstract class of `IAccountant` to implement our custom LaplacianAccountant"
      ],
      "metadata": {
        "id": "zWVplM_8pwR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opacus.accountants.accountant import IAccountant\n",
        "\n",
        "class LaplacianAccountant(IAccountant):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def step(self, *, noise_multiplier: float, sample_rate: float):\n",
        "        if len(self.history) >= 1:\n",
        "            step_num = self.history[-1][-1]\n",
        "            self.history.append((noise_multiplier, sample_rate, step_num + 1))\n",
        "        else:\n",
        "            self.history = [(noise_multiplier, sample_rate, 1)]\n",
        "\n",
        "    def get_epsilon(self, delta: float = 0, poisson: bool = True) -> float:\n",
        "        \"\"\"\n",
        "        Return privacy budget (epsilon) expended so far.\n",
        "\n",
        "        Args:\n",
        "            delta: this is always 0 for Laplace mechanism\n",
        "            poisson: ``True`` is input batches was sampled via Poisson sampling,\n",
        "                ``False`` otherwise\n",
        "        \"\"\"\n",
        "        assert delta == 0, \"Laplace mechanism does not support delta\"\n",
        "        assert poisson, \"Our mechanism only supports Poisson sampling\"\n",
        "\n",
        "        return compute_eps(self.history)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.history)\n",
        "\n",
        "    @classmethod\n",
        "    def mechanism(cls) -> str:\n",
        "        return \"lap\""
      ],
      "metadata": {
        "id": "-yyLUfLDPkV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to do is to initialize the accountant object and attach it to track `DPOptimizer`"
      ],
      "metadata": {
        "id": "rjDB_Ouyp-0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accountant = LaplacianAccountant()\n",
        "optimizer.attach_step_hook(accountant.get_optimizer_hook_fn(sample_rate=sample_rate))"
      ],
      "metadata": {
        "id": "Aym7IHtnrbtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Finally, train the model!"
      ],
      "metadata": {
        "id": "TlVKMiCrKEsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us write code to train for 1 epoch"
      ],
      "metadata": {
        "id": "TfwxVDIsiPrw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk_txHXGKEt7"
      },
      "source": [
        "import numpy as np\n",
        "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
        "\n",
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "def train(backbone, head, optimizer, train_loader, epoch=1, verbose=True):\n",
        "    top1_accs = []\n",
        "    losses = []\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    head.train()\n",
        "    backbone.train()\n",
        "    with BatchMemoryManager(\n",
        "        data_loader=train_loader,\n",
        "        max_physical_batch_size=BATCH_SIZE,\n",
        "        optimizer=optimizer\n",
        "    ) as memory_safe_data_loader:\n",
        "        for i, (x, y) in  tqdm(enumerate(memory_safe_data_loader), desc=\"Step\", unit=\"step\"):\n",
        "            optimizer.zero_grad()\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # compute output\n",
        "            with torch.no_grad():\n",
        "                x = backbone(x)\n",
        "\n",
        "            logits = head(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "            preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "            labels = y.detach().cpu().numpy()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc = accuracy(preds, labels)\n",
        "            losses.append(loss.item())\n",
        "            top1_accs.append(acc)\n",
        "\n",
        "            # compute update\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if i % 50 == 0 and verbose:\n",
        "                epsilon = accountant.get_epsilon()\n",
        "                print(\n",
        "                    f\"\\tTrain Epoch: {epoch} \\t\"\n",
        "                    f\"Step: {i} \\t\"\n",
        "                    f\"Loss: {np.mean(losses):.6f} \"\n",
        "                    f\"Acc@1: {np.mean(top1_accs) * 100:.6f} \"\n",
        "                    f\"(ε = {epsilon:.2f})\"\n",
        "                )\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will create a function to validate on our test dataset"
      ],
      "metadata": {
        "id": "lFsMUWFsbCon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(head, backbone, test_loader):\n",
        "    head.eval()\n",
        "    backbone.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            x = backbone(x)\n",
        "            output = head(x)\n",
        "            loss = criterion(output, y)\n",
        "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
        "            labels = y.detach().cpu().numpy()\n",
        "            acc = accuracy(preds, labels)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            top1_acc.append(acc)\n",
        "\n",
        "    top1_avg = np.mean(top1_acc)\n",
        "\n",
        "    print(\n",
        "        f\"\\tTest set:\"\n",
        "        f\"Loss: {np.mean(losses):.6f} \"\n",
        "        f\"Acc: {top1_avg * 100:.6f} \"\n",
        "    )\n",
        "    return np.mean(top1_acc)"
      ],
      "metadata": {
        "id": "hptMC1l5nN5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtUGQeGhF-pg"
      },
      "source": [
        "for epoch in tqdm(range(10), desc=\"Epoch\", unit=\"epoch\"):\n",
        "    train(backbone, head, optimizer, train_loader, epoch+1)\n",
        "test(head, backbone, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4. There is a fundamental mistake in the above implementation. We clip L2 norm, whereas we should have clipped L1 norm. Fix this (2 points)\n",
        "\n",
        "To fix this, you will have to override the `clip_and_accumulate()` method of the `DPOptimizer` class when implementing `LaplaceDPOptimizer`. Copy the default implementaion from [here](https://github.com/pytorch/opacus/blob/a246aa644bc9f04aaf17faa5706efb145a7c6ac7/opacus/optimizers/optimizer.py#L429C9-L429C28) and edit what is required to change from L2 norm to L1 norm.\n",
        "\n",
        "Re-run your code. What happens to the convergence?"
      ],
      "metadata": {
        "id": "98TqHJf1so9J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NP8RBLJe_sg4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}